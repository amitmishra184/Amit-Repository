{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998e100e-cba0-4e0b-9a6c-e11419d4e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b770482-e02d-4f2c-ad5e-a7d96533f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e46840-e0cf-45b0-8d8a-b802e9b83119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatSession',\n",
       " 'GenerationConfig',\n",
       " 'GenerativeModel',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'annotations',\n",
       " 'caching',\n",
       " 'configure',\n",
       " 'create_tuned_model',\n",
       " 'delete_file',\n",
       " 'delete_tuned_model',\n",
       " 'embed_content',\n",
       " 'embed_content_async',\n",
       " 'get_base_model',\n",
       " 'get_file',\n",
       " 'get_model',\n",
       " 'get_operation',\n",
       " 'get_tuned_model',\n",
       " 'list_files',\n",
       " 'list_models',\n",
       " 'list_operations',\n",
       " 'list_tuned_models',\n",
       " 'protos',\n",
       " 'responder',\n",
       " 'string_utils',\n",
       " 'types',\n",
       " 'update_tuned_model',\n",
       " 'upload_file',\n",
       " 'utils']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b128b5-b110-4567-ac58-c4d748799c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object list_models at 0x000001B040A62110>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genai.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a14d5f-0b10-4d86-9cc0-0c2eaf6b4a51",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "\n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlist\u001b[39m(genai\u001b[38;5;241m.\u001b[39mlist_models())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\models.py:202\u001b[0m, in \u001b[0;36mlist_models\u001b[1;34m(page_size, client, request_options)\u001b[0m\n\u001b[0;32m    199\u001b[0m     request_options \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 202\u001b[0m     client \u001b[38;5;241m=\u001b[39m get_default_model_client()\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m client\u001b[38;5;241m.\u001b[39mlist_models(page_size\u001b[38;5;241m=\u001b[39mpage_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options):\n\u001b[0;32m    205\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39mto_dict(model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:372\u001b[0m, in \u001b[0;36mget_default_model_client\u001b[1;34m()\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_default_model_client\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m glm\u001b[38;5;241m.\u001b[39mModelServiceAsyncClient:\n\u001b[1;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _client_manager\u001b[38;5;241m.\u001b[39mget_default_client(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:289\u001b[0m, in \u001b[0;36m_ClientManager.get_default_client\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients\u001b[38;5;241m.\u001b[39mget(name)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_client(name)\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclients[name] \u001b[38;5;241m=\u001b[39m client\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:249\u001b[0m, in \u001b[0;36m_ClientManager.make_client\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ga_exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    243\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  No API_KEY or ADC found. Please either:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    248\u001b[0m     )\n\u001b[1;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_metadata:\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\generativeai\\client.py:241\u001b[0m, in \u001b[0;36m_ClientManager.make_client\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch_colab_gce_credentials():\n\u001b[1;32m--> 241\u001b[0m         client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient_config)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ga_exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    243\u001b[0m     e\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  No API_KEY or ADC found. Please either:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Set the `GOOGLE_API_KEY` environment variable.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    248\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\client.py:661\u001b[0m, in \u001b[0;36mModelServiceClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    653\u001b[0m     transport_init: Union[\n\u001b[0;32m    654\u001b[0m         Type[ModelServiceTransport], Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ModelServiceTransport]\n\u001b[0;32m    655\u001b[0m     ] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, ModelServiceTransport], transport)\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m transport_init(\n\u001b[0;32m    662\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    663\u001b[0m         credentials_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_options\u001b[38;5;241m.\u001b[39mcredentials_file,\n\u001b[0;32m    664\u001b[0m         host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_endpoint,\n\u001b[0;32m    665\u001b[0m         scopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_options\u001b[38;5;241m.\u001b[39mscopes,\n\u001b[0;32m    666\u001b[0m         client_cert_source_for_mtls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_cert_source,\n\u001b[0;32m    667\u001b[0m         quota_project_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_options\u001b[38;5;241m.\u001b[39mquota_project_id,\n\u001b[0;32m    668\u001b[0m         client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[0;32m    669\u001b[0m         always_use_jwt_access\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    670\u001b[0m         api_audience\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_options\u001b[38;5;241m.\u001b[39mapi_audience,\n\u001b[0;32m    671\u001b[0m     )\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport):\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(\n\u001b[0;32m    675\u001b[0m         std_logging\u001b[38;5;241m.\u001b[39mDEBUG\n\u001b[0;32m    676\u001b[0m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\transports\\grpc.py:239\u001b[0m, in \u001b[0;36mModelServiceGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    235\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m    236\u001b[0m             )\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    240\u001b[0m     host\u001b[38;5;241m=\u001b[39mhost,\n\u001b[0;32m    241\u001b[0m     credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    242\u001b[0m     credentials_file\u001b[38;5;241m=\u001b[39mcredentials_file,\n\u001b[0;32m    243\u001b[0m     scopes\u001b[38;5;241m=\u001b[39mscopes,\n\u001b[0;32m    244\u001b[0m     quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id,\n\u001b[0;32m    245\u001b[0m     client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[0;32m    246\u001b[0m     always_use_jwt_access\u001b[38;5;241m=\u001b[39malways_use_jwt_access,\n\u001b[0;32m    247\u001b[0m     api_audience\u001b[38;5;241m=\u001b[39mapi_audience,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     channel_init \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\model_service\\transports\\base.py:103\u001b[0m, in \u001b[0;36mModelServiceTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m    100\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_credentials:\n\u001b[1;32m--> 103\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mdefault(\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\_default.py:739\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    731\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    735\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[0;32m    736\u001b[0m             )\n\u001b[0;32m    737\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: \n  No API_KEY or ADC found. Please either:\n    - Set the `GOOGLE_API_KEY` environment variable.\n    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information."
     ]
    }
   ],
   "source": [
    "list(genai.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce32e39-62a3-4554-a418-0fca8fca4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyDisGEMhYd--B29jz7CXFS3pRTn4vpjGj0\")\n",
    "#model = genai.GenerativeModel(\"gemini-pro\")\n",
    "#response = model.generate_content(\"Explain Python in one line\")\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2937fcb-ecae-4ae1-b68f-58af4b84d5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash',\n",
       "       description=('Stable version of Gemini 2.5 Flash, our mid-size multimodal model that '\n",
       "                    'supports up to 1 million tokens, released in June of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro',\n",
       "       base_model_id='',\n",
       "       version='2.5',\n",
       "       display_name='Gemini 2.5 Pro',\n",
       "       description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.0-flash-exp',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash Experimental',\n",
       "       description='Gemini 2.0 Flash Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash',\n",
       "       description='Gemini 2.0 Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash 001',\n",
       "       description=('Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model '\n",
       "                    'for scaling across diverse tasks, released in January of 2025.'),\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-exp-image-generation',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite 001',\n",
       "       description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Gemini 2.0 Flash-Lite',\n",
       "       description='Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-2.0-flash-lite-preview',\n",
       "       base_model_id='',\n",
       "       version='preview-02-05',\n",
       "       display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "       description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/gemini-exp-1206',\n",
       "       base_model_id='',\n",
       "       version='2.5-exp-03-25',\n",
       "       display_name='Gemini Experimental 1206',\n",
       "       description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-exp-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Preview TTS',\n",
       "       description='Gemini 2.5 Flash Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-pro-preview-tts',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-pro-preview-tts-2025-05-19',\n",
       "       display_name='Gemini 2.5 Pro Preview TTS',\n",
       "       description='Gemini 2.5 Pro Preview TTS',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=16384,\n",
       "       supported_generation_methods=['countTokens', 'generateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-1b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 1B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 4B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-12b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 12B',\n",
       "       description='',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3-27b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3 27B',\n",
       "       description='',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e4b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E4B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemma-3n-e2b-it',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemma 3n E2B',\n",
       "       description='',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash Latest',\n",
       "       display_name='Gemini Flash Latest',\n",
       "       description='Latest release of Gemini Flash',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-flash-lite-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Flash-Lite Latest',\n",
       "       display_name='Gemini Flash-Lite Latest',\n",
       "       description='Latest release of Gemini Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini Pro Latest',\n",
       "       display_name='Gemini Pro Latest',\n",
       "       description='Latest release of Gemini Pro',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 2.5 Flash-Lite',\n",
       "       description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-image-preview',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Nano Banana',\n",
       "       description='Gemini 2.5 Flash Preview Image',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-image',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Nano Banana',\n",
       "       description='Gemini 2.5 Flash Preview Image',\n",
       "       input_token_limit=32768,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Preview 09-2025',\n",
       "       display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       description='Gemini 2.5 Flash Preview Sep 2025',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='2.5-preview-09-25',\n",
       "       display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
       "       description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3-pro-preview-11-2025',\n",
       "       display_name='Gemini 3 Pro Preview',\n",
       "       description='Gemini 3 Pro Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent',\n",
       "                                     'countTokens',\n",
       "                                     'createCachedContent',\n",
       "                                     'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-3-pro-image-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/nano-banana-pro-preview',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Nano Banana Pro',\n",
       "       description='Gemini 3 Pro Image Preview',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=32768,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'batchGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=1.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-robotics-er-1.5-preview',\n",
       "       base_model_id='',\n",
       "       version='1.5-preview',\n",
       "       display_name='Gemini Robotics-ER 1.5 Preview',\n",
       "       description='Gemini Robotics-ER 1.5 Preview',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-computer-use-preview-10-2025',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       description='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/deep-research-pro-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='deepthink-exp-05-20',\n",
       "       display_name='Deep Research Pro Preview (Dec-12-2025)',\n",
       "       description='Preview release (December 12th, 2025) of Deep Research Pro',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=65536,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp-03-07',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental 03-07',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-exp',\n",
       "       base_model_id='',\n",
       "       version='exp-03-07',\n",
       "       display_name='Gemini Embedding Experimental',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=8192,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40),\n",
       " Model(name='models/imagen-4.0-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 (Preview)',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-preview-06-06',\n",
       "       base_model_id='',\n",
       "       version='01',\n",
       "       display_name='Imagen 4 Ultra (Preview)',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4',\n",
       "       description='Vertex served Imagen 4.0 model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-ultra-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Ultra',\n",
       "       description='Vertex served Imagen 4.0 ultra model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/imagen-4.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Imagen 4 Fast',\n",
       "       description='Vertex served Imagen 4.0 Fast model',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predict'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-2.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='2.0',\n",
       "       display_name='Veo 2',\n",
       "       description=('Vertex served Veo 2 model. Access to this model requires billing to be '\n",
       "                    'enabled on the associated Google Cloud Platform account. Please visit '\n",
       "                    'https://console.cloud.google.com/billing to enable it.'),\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3',\n",
       "       description='Veo 3',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.0-fast-generate-001',\n",
       "       base_model_id='',\n",
       "       version='3.0',\n",
       "       display_name='Veo 3 fast',\n",
       "       description='Veo 3 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1',\n",
       "       description='Veo 3.1',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/veo-3.1-fast-generate-preview',\n",
       "       base_model_id='',\n",
       "       version='3.1',\n",
       "       display_name='Veo 3.1 fast',\n",
       "       description='Veo 3.1 fast',\n",
       "       input_token_limit=480,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['predictLongRunning'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-latest',\n",
       "       base_model_id='',\n",
       "       version='Gemini 2.5 Flash Native Audio Latest',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Latest',\n",
       "       description='Latest release of Gemini 2.5 Flash Native Audio',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-09-2025',\n",
       "       base_model_id='',\n",
       "       version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 09-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-2.5-flash-native-audio-preview-12-2025',\n",
       "       base_model_id='',\n",
       "       version='12-2025',\n",
       "       display_name='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       description='Gemini 2.5 Flash Native Audio Preview 12-2025',\n",
       "       input_token_limit=131072,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['countTokens', 'bidiGenerateContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(genai.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60dc1822-c40d-4bbc-b01a-e96303dd1d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-flash-latest\n",
      "models/gemini-flash-lite-latest\n",
      "models/gemini-pro-latest\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/gemini-2.5-flash-image\n",
      "models/gemini-2.5-flash-preview-09-2025\n",
      "models/gemini-2.5-flash-lite-preview-09-2025\n",
      "models/gemini-3-pro-preview\n",
      "models/gemini-3-pro-image-preview\n",
      "models/nano-banana-pro-preview\n",
      "models/gemini-robotics-er-1.5-preview\n",
      "models/gemini-2.5-computer-use-preview-10-2025\n",
      "models/deep-research-pro-preview-12-2025\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-001\n",
      "models/veo-3.0-fast-generate-001\n",
      "models/veo-3.1-generate-preview\n",
      "models/veo-3.1-fast-generate-preview\n",
      "models/gemini-2.5-flash-native-audio-latest\n",
      "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "for i in genai.list_models():\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32f8cace-5a29-4bcd-8449-bbcb4efe54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-2.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e54866-00d8-47cd-94e2-e4a723ba9260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_async_client',\n",
       " '_client',\n",
       " '_generation_config',\n",
       " '_get_tools_lib',\n",
       " '_model_name',\n",
       " '_prepare_request',\n",
       " '_safety_settings',\n",
       " '_system_instruction',\n",
       " '_tool_config',\n",
       " '_tools',\n",
       " 'cached_content',\n",
       " 'count_tokens',\n",
       " 'count_tokens_async',\n",
       " 'from_cached_content',\n",
       " 'generate_content',\n",
       " 'generate_content_async',\n",
       " 'model_name',\n",
       " 'start_chat']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07c8646b-74a2-4e38-96ad-8880155f9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content('Where is messi footballer today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cbef7b5-161d-47ff-8638-e283b745b435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_chunks',\n",
       " '_done',\n",
       " '_error',\n",
       " '_iterator',\n",
       " '_result',\n",
       " 'candidates',\n",
       " 'from_iterator',\n",
       " 'from_response',\n",
       " 'model_version',\n",
       " 'parts',\n",
       " 'prompt_feedback',\n",
       " 'resolve',\n",
       " 'text',\n",
       " 'to_dict',\n",
       " 'usage_metadata']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a123a850-f6da-4cf2-9d8c-4befd45e5e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lionel Messi currently plays for **Inter Miami CF** in Major League Soccer (MLS), which is based in **Miami, Florida, USA**.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba4a07f-e075-4a4e-a977-3a6ab9369e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lionel Messi currently plays for **Inter Miami CF** in Major League Soccer (MLS), which is based in **Miami, Florida, USA**.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee580a-1770-4a56-9f5a-4962a099885f",
   "metadata": {},
   "source": [
    "**Using langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfb61466-51e6-40a5-ade8-2aceb877e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c8598a8-2ee4-446d-a9b1-178579c3d7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ChatGoogleGenerativeAI',\n",
       " 'ComputerUse',\n",
       " 'Environment',\n",
       " 'GoogleGenerativeAI',\n",
       " 'GoogleGenerativeAIEmbeddings',\n",
       " 'HarmBlockThreshold',\n",
       " 'HarmCategory',\n",
       " 'MediaResolution',\n",
       " 'Modality',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_common',\n",
       " '_compat',\n",
       " '_enums',\n",
       " '_function_utils',\n",
       " '_image_utils',\n",
       " 'chat_models',\n",
       " 'create_context_cache',\n",
       " 'data',\n",
       " 'embeddings',\n",
       " 'llms',\n",
       " 'utils']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain_google_genai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "038d2569-a738-472b-9654-d5c9d72ae821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631cb779-0a7a-4e80-aeab-9dc481c4e367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_computed_fields__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_on_complete__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_setattr_handlers__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__replace__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_recursion__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall_with_config',\n",
       " '_add_response_parameters',\n",
       " '_agenerate',\n",
       " '_agenerate_with_cache',\n",
       " '_astream',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_build_base_generation_config',\n",
       " '_build_request_config',\n",
       " '_build_thinking_config',\n",
       " '_calculate_keys',\n",
       " '_call_async',\n",
       " '_call_with_config',\n",
       " '_combine_llm_outputs',\n",
       " '_convert_cached_generations',\n",
       " '_convert_input',\n",
       " '_copy_and_set_values',\n",
       " '_determine_backend',\n",
       " '_extract_tool_names',\n",
       " '_filter_messages',\n",
       " '_format_safety_settings',\n",
       " '_format_tools',\n",
       " '_generate',\n",
       " '_generate_with_cache',\n",
       " '_get_invocation_params',\n",
       " '_get_llm_string',\n",
       " '_get_ls_params',\n",
       " '_get_value',\n",
       " '_identifying_params',\n",
       " '_iter',\n",
       " '_llm_type',\n",
       " '_merge_generation_config',\n",
       " '_prepare_params',\n",
       " '_prepare_request',\n",
       " '_process_tool_config',\n",
       " '_resolve_project_from_credentials',\n",
       " '_serialized',\n",
       " '_set_model_profile',\n",
       " '_setattr_handler',\n",
       " '_should_stream',\n",
       " '_stream',\n",
       " '_supports_code_execution',\n",
       " '_supports_thinking',\n",
       " '_supports_tool_choice',\n",
       " '_transform_stream_with_config',\n",
       " '_validate_and_add_response_schema',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'agenerate',\n",
       " 'agenerate_prompt',\n",
       " 'ainvoke',\n",
       " 'as_tool',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'async_client',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'bind_tools',\n",
       " 'build_extra',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'generate',\n",
       " 'generate_prompt',\n",
       " 'get_config_jsonschema',\n",
       " 'get_graph',\n",
       " 'get_input_jsonschema',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_num_tokens',\n",
       " 'get_num_tokens_from_messages',\n",
       " 'get_output_jsonschema',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'get_token_ids',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'map',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_environment',\n",
       " 'with_alisteners',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_structured_output',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ChatGoogleGenerativeAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32e1016a-4178-4007-bf36-65cca265cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
    "                      api_key= 'AIzaSyDisGEMhYd--B29jz7CXFS3pRTn4vpjGj0',\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7be9b512-e3c2-4d63-b8d5-c6a44394641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile={'max_input_tokens': 1048576, 'max_output_tokens': 65536, 'image_inputs': True, 'audio_inputs': True, 'pdf_inputs': True, 'video_inputs': True, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'image_tool_message': True, 'tool_choice': True} google_api_key=SecretStr('**********') model='gemini-2.5-flash' client=<google.genai.client.Client object at 0x000001B0469D90D0> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdd18078-8d5e-4d82-a21a-9d5745b7b050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Choosing the top 5 Indian cricketers of all time is a challenging task, as it involves comparing players across different eras, formats, and with varying skill sets. However, based on their statistical dominance, impact on the game, leadership, and lasting legacy, here\\'s a widely accepted list:\\n\\n1.  **Sachin Tendulkar:**\\n    *   **Why he\\'s on the list:** Widely regarded as the \"God of Cricket,\" Tendulkar holds almost every major batting record in international cricket, including most runs in both Tests and ODIs, and the most international centuries (100). His longevity (24 years) and ability to dominate bowlers across generations and conditions are unparalleled. He carried the hopes of a nation for over two decades.\\n\\n2.  **Virat Kohli:**\\n    *   **Why he\\'s on the list:** A modern-day batting colossus, Kohli\\'s consistency across all three formats is staggering. He is a prolific run-scorer, especially in ODIs where he has redefined chasing targets. He is rapidly accumulating records, including being the fastest to many ODI milestones, and is often compared to Sachin Tendulkar for his run-scoring prowess and ability to perform under pressure. His captaincy also led India to significant success in Tests.\\n\\n3.  **Kapil Dev:**\\n    *   **Why he\\'s on the list:** India\\'s greatest fast-bowling all-rounder, Kapil Dev was a genuine match-winner with both bat and ball. He was the first player in Test history to take 400 wickets and score 5,000 runs. His most significant achievement was leading India to its first-ever World Cup victory in 1983, a transformative moment for Indian cricket.\\n\\n4.  **Sunil Gavaskar:**\\n    *   **Why he\\'s on the list:** The original \"Little Master,\" Gavaskar was a pioneering opening batsman who set the standard for Test cricket in India. He was the first batsman to score 10,000 runs in Test cricket and held the record for most Test centuries (34) for a long time. He achieved this against the fearsome fast bowlers of the West Indies without a helmet, showcasing immense courage, technique, and resilience.\\n\\n5.  **MS Dhoni:**\\n    *   **Why he\\'s on the list:** India\\'s most successful captain, Dhoni led India to all three major ICC trophies: the 2007 T20 World Cup, the 2011 ODI World Cup, and the 2013 Champions Trophy. He was an astute leader, an exceptional finisher with the bat, and one of the best wicket-keepers in the world. His calm demeanor under pressure and strategic brilliance made him a unique and indispensable force in Indian cricket.\\n\\n**Honorable Mentions:** Rahul Dravid (\"The Wall\" for his resilience and record in Tests), Anil Kumble (India\\'s leading Test wicket-taker), Sourav Ganguly (a transformative captain and aggressive left-handed batsman).', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b17ef-8e1c-7fe0-aa4b-d712bc07bf75-0', usage_metadata={'input_tokens': 11, 'output_tokens': 1962, 'total_tokens': 1973, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1330}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('Who are top 5 indian cricketer of all time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e903e0-50f6-40a2-b2cd-ea9c20ddf34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
